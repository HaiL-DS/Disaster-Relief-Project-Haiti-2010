---
title: "Copy for Incorporation - Virginia's Code"
author: "Virginia Brame"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup}
knitr::opts_chunk$set(
  echo = FALSE,
  cache = TRUE,
  autodep = TRUE,
  fig.align = "center",
  fig.pos = "H",
  out.width = "100%"
)
```

```{r libraries}
#| cache: FALSE
#| warning: FALSE
#| message: FALSE
library(tidyverse)
library(tidymodels)
library(discrim)
library(patchwork)
library(probably)
library(gridExtra)
library(kableExtra)
library(kknn)
#library(xgboost) # in case I try this too
library(randomForest)
library(themis)  # step_downsample
library(ranger)  # random forest engine
```

```{r}
library(doParallel)
cl <- makePSOCKcluster(10)
registerDoParallel(cl)
```
### Recap

Not needing to recreate the data exploration completed in part I, we will begin exploring a number of regression models and assess their predictive ability with the RGB training and holdout sets, revisiting previous work as needed for explanation.

It is worth noting, once again, that the "holdout" set we will refer to is not a true holdout set in the sense that it is not a split from a unified data set. It is completely new data from a different geographic coordinates with an unrelated color composition. As such, there was no stratification with respect for the response variable density and we have already learned in part I that the response variable in this holdout data set is even more rare than in the training data set, leading to a challenging an unbalanced classification problem.

## Data loading and wrangling

Since we are only interested in the level of "Blue Tarp", I create a new variable `BT` with only two classes, i.e., "TRUE" for "Blue Tarp" and "FALSE" for everything else.

```{r holdout data processing}
#| message: FALSE
#| warning: FALSE

col_names <- c('ID','X','Y','Map X','Map Y','Lat','Lon','Red','Green','Blue')

blue_files <- c(
  "orthovnir069_ROI_Blue_Tarps.txt",
  "orthovnir067_ROI_Blue_Tarps.txt",
  "orthovnir078_ROI_Blue_Tarps.txt"
)

non_blue_files <- c(
  "orthovnir057_ROI_NON_Blue_Tarps.txt",
  "orthovnir078_ROI_NON_Blue_Tarps.txt",
  "orthovnir067_ROI_NOT_Blue_Tarps.txt",
  "orthovnir069_ROI_NOT_Blue_Tarps.txt"
)

blue_data <- map_dfr(blue_files, ~ 
  read_table(.x, comment = ";", col_names = col_names, col_types = cols(
    `Map X` = col_double(),
    `Map Y` = col_double(),
    Red = col_integer(),
    Green = col_integer(),
    Blue = col_integer()
  )) %>% 
    select(`Map X`, `Map Y`, Red, Green, Blue) %>% 
    mutate(BT = "TRUE")
)

non_blue_data <- map_dfr(non_blue_files, ~ 
  read_table(.x, comment = ";", col_names = col_names, col_types = cols(
    `Map X` = col_double(),
    `Map Y` = col_double(),
    Red = col_integer(),
    Green = col_integer(),
    Blue = col_integer()
  )) %>% 
    select(`Map X`, `Map Y`, Red, Green, Blue) %>% 
    mutate(BT = "FALSE")
)
```

### RGB Data

#### RGB Training 

```{r training data processing}
#| message: FALSE

train_data <- read_csv("HaitiPixels.csv") %>%
  mutate(BT = factor(if_else(Class == "Blue Tarp", "TRUE", "FALSE"), levels = c("TRUE", "FALSE"))) %>%
  select(Red, Green, Blue, BT)
```

#### RGB Holdout
#### 0.2 sample: RGB-Holdout
```{r}
# RGB holdout data
holdout_data <- bind_rows(blue_data, non_blue_data) %>% 
  mutate(BT = factor(BT, levels = c("TRUE", "FALSE")))
# 0.2 stratified dample of RGB holdout
strat_holdout_sample <- holdout_data %>% 
  group_by(BT) %>% 
  sample_frac(size = 0.2)
```

### EV DATA - engineered variables
#### EV Training
```{r}
# EV training set
train_engineered <- readRDS("train_data _engineered_vars.rds")
glimpse(train_engineered)
```
```{r}
sum(is.na(train_engineered))
```

#### EV Holdout
```{r}
# EV holdout set
# removing NAs for PCA
holdout_engineered <- readRDS("holdout_data_engineered_vars.rds") %>% 
  na.omit()
glimpse(holdout_engineered)
```

#### 0.2 sample: EV-Holdout
```{r}
# 0.2 stratified dample of RGB holdout
holdout_engineered_stratsample <- holdout_engineered %>% 
  na.omit() %>% 
  group_by(BT) %>% 
  sample_frac(size = 0.2)
```

```{r}
# verifying zero NA
sum(is.na(holdout_engineered))
sum(is.na(holdout_engineered_stratsample))
```


## BUILDING the MODELS

**In this section of the analysis we are exploring the following models:**

-   k-nearest neighbors with undersampling-SMOTE hybrid approach + PCA:
      - in RGB
      - with engineered variables dataset
-   random forest (engine = ranger) with undersampling-SMOTE hybrid approach + PCA:
      - in RGB
      - with engineered variables dataset

### recipes and formulas:

Initial attempts to train a tuned k-nearest neighbors model using the full dataset were unsuccessful simply because the computational needs exceeded both our local machines as well as Rivanna.

We pivoted to downsampling the data and combining it with SMOTE oversampling of the response variable to address the unbalanced nature of the data set.

#### RGB 
```{r}
rgb_formula <- BT ~ Red + Green + Blue

# https://themis.tidymodels.org/reference/step_downsample.html

recipe_downsamp_rgb <- recipe(rgb_formula, data = train_data) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_downsample(BT, under_ratio = 3, seed = 6020) 

# https://themis.tidymodels.org/reference/step_smote.html

recipe_hybrid_rgb <- recipe(rgb_formula, data = train_data) %>%  # this is what I retained
  step_normalize(all_numeric_predictors()) %>% 
  step_downsample(BT, under_ratio = 6, seed = 6020) %>% 
  step_smote(BT, over_ratio = 0.33)
```

#### EV_RDS: Data with Engineered Variables

```{r}
ev_formula <- BT ~ Red + Green + Blue + Luminance + a + b + Hue + Saturation + Value + Red_Prop + Green_Prop + Blue_Prop + Dispersion + Hue_Shifted + Red_9 + Green_9 + Blue_9 + Luminance_9 + a_9 + b_9 + Hue_9 + Saturation_9 + Value_9 + Red_Prop_9 + Green_Prop_9 + Blue_Prop_9 + Dispersion_9 + Hue_Shifted_9

# https://themis.tidymodels.org/reference/step_smote.html

recipe_hybrid_ev <- recipe(ev_formula, data = train_engineered) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_downsample(BT, under_ratio = 6, seed = 6020) %>% 
  step_smote(BT, over_ratio = 0.33)
```

```{r}
# before and after down_sampling
g1 <- ggplot(train_data, aes(x=Red, y=Blue, color=BT))+
  geom_point(alpha=0.3, size=0.1)+
  labs(title="full dataset")

g2 <- recipe_downsamp_rgb %>% 
  prep() %>% 
  bake(new_data = NULL) %>% 
  ggplot(aes(x=Red, y=Blue, color=BT))+
  geom_point(alpha=0.3, size=0.1)+
  labs(title="downsample (1:3)")

g3 <- recipe_hybrid_rgb %>% 
  prep() %>% 
  bake(new_data = NULL) %>% 
  ggplot(aes(x=Red, y=Blue, color=BT))+
  geom_point(alpha=0.3, size=0.3)+
  labs(title="downsample + SMOTE", subtitle ="(1:6 + 0.33) hybrid on RGB")   

g4 <- recipe_hybrid_ev %>% 
  prep() %>% 
  bake(new_data = NULL) %>% 
  ggplot(aes(x=Red, y=Blue, color=BT))+
  geom_point(alpha=0.3, size=0.3)+
  labs(title="downsample + SMOTE", subtitle ="(1:6 + 0.33) hybrid with engineered variables")   

(g1 + g2) / (g3 + g4)
```

#### PCA recipes

```{r}
# https://gedeck.github.io/DS-6030/livesessions/module-2-pca.pdf

# recipe: downsampling/SMOTE + pca
recipe_hybrid_pca_rgb <- recipe(data=train_data, formula = rgb_formula) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_downsample(BT, under_ratio = 3, seed = 6020) %>% 
  step_pca(all_numeric_predictors(), num_comp = tune())  # num_comp tune HERE

# engineered recipe: downsampling/SMOTE + pca
recipe_hybrid_pca_ev <- recipe(data=train_engineered, formula = ev_formula) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_downsample(BT, under_ratio = 3, seed = 5999) %>% 
  step_pca(all_numeric_predictors(), num_comp = tune())  # num_comp tune HERE
```

#### About cross-validation

A 10-fold cross-validation procedure was implemented using stratified sampling to ensure that each fold maintained the same proportion of positive (`BT = TRUE`) and negative (`BT = FALSE`) cases as the full dataset. The cross-validation results allowed us to tune our hyperparameters, selecting the optimal values for peak model performance.

Cross-validation performance was evaluated using ROC-AUC as the primary metric.

#### preparing cross-validation

```{r}
set.seed(1)

resamples <- vfold_cv(train_data, v=10, strata = BT)
resamples_ev <- vfold_cv(train_engineered, v=10, strata = BT)
cv_control <- control_resamples(save_pred = TRUE)

# https://yardstick.tidymodels.org/reference/bal_accuracy.html

cv_metrics <- metric_set(       # use after tuning: test the performance on the training data
  accuracy, 
  bal_accuracy, 
  roc_auc, 
  f_meas, 
  sens
  )
```

# K-NEAREST NEIGHBORS MODEL
## KNN -- RBG
#### Tuning

The k-nearest neighbors model uses principal component analysis to explore our parameter space in a different way.

SMOTE/Downsampling Hybrid Method (+ tune_grid --\> tune_bayes)
#### specs
#### workflows
```{r, message=FALSE}
# https://dials.tidymodels.org/reference/dist_power.html
# determines how "nearness" is calculated
# straight line or "city block" distance

# https://dials.tidymodels.org/reference/weight_func.html
# uniform vs distance (closer neighbors have more influence)


# KNN with PCA: specify models & parameters to tune
knn_spec_pca_hybrid <- nearest_neighbor(
  engine = "kknn",
  mode = "classification",
  neighbors = tune(),
  weight_func = tune(), # uniform vs distance (closer neighbors have more influence)
  dist_power = tune()
)
  
# KNN with PCA: define workflow
knn_pca_hybrid_wf <- workflow() %>% 
  add_recipe(recipe_hybrid_pca_rgb) %>% 
  add_model(knn_spec_pca_hybrid)


# tuning model parameters WITH PCA
tune_params_knn_phg <- extract_parameter_set_dials(knn_pca_hybrid_wf) %>% 
  update(
    neighbors = neighbors(range = c(3,150)),
    weight_func = weight_func(values = values_weight_func),
    dist_power = dist_power(range = c(1,2)),  # 1 = Manhattan dist, 2 = Euclidean
    num_comp = num_comp(range = c(1,10))
    )

set.seed(2)
# KNN PCA HYBRID tuning results
tune_results_1_knn <- tune_grid(  # regular grid
  knn_pca_hybrid_wf,
  resamples = resamples, 
  control = cv_control, 
  grid = grid_random(tune_params_knn_phg, size = 25)
)

tune_results_2_knn <- tune_bayes(  # bayes initiated with regular tuning obj
  knn_pca_hybrid_wf,
  resamples = resamples, 
  control = control_bayes(verbose = TRUE), # bayes needs control_bayes() obj!!
  param_info = tune_params_knn_phg,
  initial = tune_results_1_knn,
  iter = 30
)
```

```{r}
autoplot(tune_results_2_knn)
```

```{r}
# best parameters
best_params_knn <- show_best(tune_results_2_knn, metric = "roc_auc", n=6)
best_params_knn
```

#### best hyperparameters

```{r}
select_best(tune_results_2_knn, metric = "roc_auc")%>% 
  kable(caption="<center>Best Hyperparameters for KNN (pgh)") %>% 
  kableExtra::kable_styling(full_width = FALSE)
```

## KNN -- EV
#### TUNING, spec, wf
```{r, message=FALSE}
knn_spec_pca_hybrid_ev <- nearest_neighbor(
  engine = "kknn",
  mode = "classification",
  neighbors = tune(),
  weight_func = tune(), # uniform vs distance (closer neighbors have more influence)
  dist_power = tune()
)
  
knn_pca_hybrid_wf_ev <- workflow() %>% 
  add_recipe(recipe_hybrid_pca_ev) %>% 
  add_model(knn_spec_pca_hybrid_ev)

tune_params_knn_phg_ev <- extract_parameter_set_dials(knn_pca_hybrid_wf_ev) %>% 
  update(
    neighbors = neighbors(range = c(3,150)),
    weight_func = weight_func(values = values_weight_func),
    dist_power = dist_power(range = c(1,2)),  # 1 = Manhattan dist, 2 = Euclidean
    num_comp = num_comp(range = c(1,22))
    )

set.seed(2)

tune_results_1_knn_ev <- tune_grid(  
  knn_pca_hybrid_wf_ev,
  resamples = resamples_ev, 
  control = cv_control, 
  grid = grid_random(tune_params_knn_phg_ev, size = 25)
)

tune_results_2_knn_ev <- tune_bayes( 
  knn_pca_hybrid_wf_ev,
  resamples = resamples_ev, 
  control = control_bayes(verbose = TRUE),
  param_info = tune_params_knn_phg_ev,
  initial = tune_results_1_knn_ev,
  iter = 30
)
```

```{r}
autoplot(tune_results_2_knn_ev)
```

```{r}
# best parameters
best_params_knn_ev <- show_best(tune_results_2_knn_ev, metric = "roc_auc", n=6)
best_params_knn_ev
```

#### best hyperparameters

```{r}
select_best(tune_results_2_knn_ev, metric = "roc_auc")%>% 
  kable(caption="<center>Best Hyperparameters for KNN with EV") %>% 
  kableExtra::kable_styling(full_width = FALSE)
```

# RANDOM FOREST MODEL (engine: ranger)

**Note about the use of PCA in Random Forest**
For the benefit of comparison, I have run some essential steps without the PCA element.  
All the steps and visualizations were not repeated (for brevity), but it felt important to have resutls to compare and see the benefit of PCA in numbers.
All essential steps were repeated and you will find in the code.

## RF -- RGB
#### TUNING, specs , wf

with PCA, SMOTE/Downsampling Hybrid Method (+ tune_grid --\> tune_bayes)
```{r, message=FALSE}
# https://parsnip.tidymodels.org/reference/details_rand_forest_ranger.html

# model specs
rf_spec_pgh <- rand_forest(
  trees = tune(),  # default 500
  mtry = tune(),   # default floor(sqrt(ncol(x)))
  min_n = tune(),  # default 10 in classification 
) %>% 
  set_engine("ranger") %>% 
  set_mode("classification") %>% 
  translate()

# define workflow
rf_pgh_wf <- workflow() %>% 
  add_recipe(recipe_hybrid_pca_rgb) %>% 
  add_model(rf_spec_pgh)

# tuning RF model parameters
tune_params_pgh_rf <- extract_parameter_set_dials(rf_pgh_wf) %>% 
  update(
    trees = trees(range = c(50, 500)), 
    mtry = mtry(range = c(1,3)),  # only three parameters (mtry=3 would be bagged trees)
    min_n = min_n(range = c(1,75)),
    num_comp = num_comp(range = c(1,22))
  )

# RF tuning results HYBRID-BAYES-PCA
set.seed(3)
tune_results_1_rf <- tune_grid(
  rf_pgh_wf,
  resamples = resamples, 
  control = cv_control, 
  grid = grid_random(tune_params_pgh_rf, size = 25)
)

tune_results_2_rf <- tune_bayes(
  rf_pgh_wf,
  resamples = resamples, 
  control = control_bayes(verbose = TRUE), # bayes needs control_bayes() obj!!
  param_info = tune_params_pgh_rf,
  initial = tune_results_1_rf,
  iter = 30
)
```

```{r}
autoplot(tune_results_2_rf, size = 1)
```

```{r}
# best parameters
best_params_rfac_pgh <- show_best(tune_results_2_rf, metric = "accuracy", n=1)
best_params_rfbr_pgh <- show_best(tune_results_2_rf, metric = "brier_class", n=1)
best_params_rfroc_pgh <- show_best(tune_results_2_rf, metric = "roc_auc", n=1)

best_params_all_rf_pgh <- bind_rows(best_params_rfac_pgh, best_params_rfbr_pgh, best_params_rfroc_pgh) %>% 
  select(-.estimator, -n, -.config, -std_err)

best_params_all_rf_pgh %>% 
  kable(caption= "<center>RF Hyperparameter Values by Metric", digits = 4) %>% 
  kableExtra::kable_styling(full_width = FALSE)
```

#### best hyperparameters:

```{r}
select_best(tune_results_2_rf, metric = "roc_auc") %>% 
  kable(caption="<center>Best Hyperparameters for RF (pgh)") %>% 
  kableExtra::kable_styling(full_width = FALSE)
  
```

## RF -- EV
#### TUNING, specs, wf
```{r, message=FALSE}
# model specs
rf_spec_pgh <- rand_forest(
  trees = tune(),  # default 500
  mtry = tune(),   # default floor(sqrt(ncol(x)))
  min_n = tune(),  # default 10 in classification 
) %>% 
  set_engine("ranger") %>% 
  set_mode("classification") %>% 
  translate()
  
rf_pca_hybrid_wf_ev <- workflow() %>% 
  add_recipe(recipe_hybrid_pca_ev) %>% 
  add_model(rf_spec_pgh)

tune_params_rf_phg_ev <- extract_parameter_set_dials(rf_pgh_wf) %>% 
  update(
    trees = trees(range = c(50, 500)), 
    mtry = mtry(range = c(1,3)),  # only three parameters (mtry=3 would be bagged trees)
    min_n = min_n(range = c(1,75)),
    num_comp = num_comp(range = c(1,22))
  )

set.seed(2)

tune_results_1_rf_ev <- tune_grid(  
  rf_pca_hybrid_wf_ev,
  resamples = resamples_ev, 
  control = cv_control, 
  grid = grid_random(tune_params_rf_phg_ev, size = 25)
)

tune_results_2_rf_ev <- tune_bayes( 
  rf_pca_hybrid_wf_ev,
  resamples = resamples_ev, 
  control = control_bayes(verbose = TRUE),
  param_info = tune_params_rf_phg_ev,
  initial = tune_results_1_rf_ev,
  iter = 30
)

# tuning RF model parameters
tune_params_pgh_rf <- extract_parameter_set_dials(rf_pgh_wf) %>% 
  update(
    trees = trees(range = c(50, 500)), 
    mtry = mtry(range = c(1,3)),  # only three parameters (mtry=3 would be bagged trees)
    min_n = min_n(range = c(1,75))
  )

# RF tuning results HYBRID-BAYES-PCA
set.seed(3)
tune_results_1_rf <- tune_grid(
  rf_pgh_wf,
  resamples = resamples, 
  control = cv_control, 
  grid = grid_random(tune_params_pgh_rf, size = 25)
)

tune_results_2_rf <- tune_bayes(
  rf_pgh_wf,
  resamples = resamples, 
  control = control_bayes(verbose = TRUE), # bayes needs control_bayes() obj!!
  param_info = tune_params_pgh_rf,
  initial = tune_results_1_rf,
  iter = 30
)
```

```{r}
autoplot(tune_results_2_rf_ev)
```

```{r}
# best parameters
best_params_rf_ev <- show_best(tune_results_2_rf_ev, metric = "roc_auc", n=6)
best_params_rf_ev
```

#### best hyperparameters

```{r}
select_best(tune_results_2_rf_ev, metric = "roc_auc")%>% 
  kable(caption="<center>Best Hyperparameters for RF (engineered variables)") %>% 
  kableExtra::kable_styling(full_width = FALSE)
```

## FINALIZING MODELS WITH TUNING RESULTS

#### finalizing KNN-RGB (hybrid + PCA)
```{r}
best_wf_knn_pgh <- knn_pca_hybrid_wf %>% 
  finalize_workflow(select_best(tune_results_2_knn, metric = "roc_auc"))

knn_pgh_cv_results <- fit_resamples(best_wf_knn_pgh, resamples, metrics = cv_metrics, control = cv_control)

knn_pgh_preds <- collect_predictions(knn_pgh_cv_results)

head(knn_pgh_preds, n=2)
```

```{r, fig.width = 3, fig.height=3}
r1 <- knn_pgh_preds %>% 
  roc_curve(truth=BT, .pred_TRUE, event_level = "first") %>% 
  autoplot()+
  labs(
    title="KNN in RGB training", 
    subtitle="with hybrid sampling + PCA")
```

#### finalizing KNN-EV (hybrid + PCA)

```{r}
best_wf_knn_pgh_ev <- knn_pca_hybrid_wf_ev %>% 
  finalize_workflow(select_best(tune_results_2_knn_ev, metric = "roc_auc"))

knn_pgh_cv_results_ev <- fit_resamples(best_wf_knn_pgh_ev, resamples_ev, metrics = cv_metrics, control = cv_control)

knn_pgh_preds_ev <- collect_predictions(knn_pgh_cv_results_ev)

head(knn_pgh_preds_ev, n=2)
```

```{r, fig.width = 3, fig.height=3}
r3 <- knn_pgh_preds_ev %>% 
  roc_curve(truth=BT, .pred_TRUE, event_level = "first") %>% 
  autoplot()+
  labs(
    title="KNN on EV (engineered vars) training", 
    subtitle="with hybrid sampling + PCA (test)")
```

#### finalizing RF-RGB (hybrid + PCA)

```{r}
best_wf_rf_pgh <- rf_pgh_wf %>% 
  finalize_workflow(select_best(tune_results_2_rf, metric = "roc_auc"))

rf_pgh_cv_results <- fit_resamples(best_wf_rf_pgh, resamples, metrics = cv_metrics, control = cv_control)

rf_pgh_preds <- collect_predictions(rf_pgh_cv_results)

head(rf_pgh_preds, n=2)
```

```{r, fig.width = 3, fig.height=3}
r2 <- rf_pgh_preds %>% 
  roc_curve(truth=BT, .pred_TRUE, event_level = "first") %>% 
  autoplot()+
  labs(
    title="Random Forest in RGB training", 
    subtitle="with hybrid sampling + PCA")
```

#### finalizing RF-EV (hybrid + PCA)
```{r}
best_wf_rf_pgh_ev <- rf_pca_hybrid_wf_ev%>% 
  finalize_workflow(select_best(tune_results_2_rf_ev, metric = "roc_auc"))

rf_pgh_cv_results_ev <- fit_resamples(best_wf_rf_pgh_ev, resamples_ev, metrics = cv_metrics, control = cv_control)

rf_pgh_preds_ev <- collect_predictions(rf_pgh_cv_results_ev)

head(rf_pgh_preds_ev, n=2)
```

```{r, fig.width = 3, fig.height=3}
r4 <- rf_pgh_preds_ev %>% 
  roc_curve(truth=BT, .pred_TRUE, event_level = "first") %>% 
  autoplot()+
  labs(
    title="Random Forest on EV (engineered vars) training", 
    subtitle="with hybrid sampling + PCA")
```

```{r}
(r1 + r2) / (r3 + r4) 
# / (r5 + r6)
```

### FITTING KNN 
#### -- RGB
```{r}
# fit tuned model on train
fitmod_knn_phg <- best_wf_knn_pgh %>% 
  fit(data = train_data)
```
#### -- EV
```{r}
fitmod_knn_phg_ev <- best_wf_knn_pgh_ev %>% 
  fit(data = train_engineered)
```

### KNN: THRESHOLD OPTIMIZATION
#### The Formulas 
**(adapted this code from Hai's part 1 code -- credit where due)**
```{r}
threshold_graph <- function(model_cv, model_name) {
  performance <- probably::threshold_perf(
    collect_predictions(model_cv),
    truth = BT,
    estimate = .pred_TRUE,
    thresholds = seq(0.05, 0.95, 0.01),
    event_level = "first",
    metrics = metric_set(accuracy, precision, f_meas, kap)
  )
  
  max_metrics <- performance %>% 
    drop_na() %>% 
    group_by(.metric) %>% 
    filter(.estimate == max(.estimate))
  
  g <- ggplot(performance, aes(x = .threshold, y = .estimate, color = .metric)) +
    geom_line() +
    geom_point(data = max_metrics, color = "black") +
    labs(title = model_name, x = "Threshold", y = "Metric value") +
    coord_cartesian(ylim = c(0.5, 1.0))
  
  thresholds <- max_metrics %>%
    select(.metric, threshold = .threshold) %>%
    deframe()
  
  return(list(graph = g, thresholds = thresholds))
}
```

```{r}
visualize_conf_mat <- function(model_cv, thresholds, metric) {
  threshold <- thresholds[metric]
  cm <- collect_predictions(model_cv) %>% 
    mutate(
      .pred_class = make_two_class_pred(.pred_TRUE, levels = c("TRUE", "FALSE"), threshold = threshold)
    ) %>% 
    conf_mat(truth = BT, estimate = .pred_class)
  
  autoplot(cm, type = "heatmap") +
    labs(title = sprintf("Threshold %.2f (%s)", threshold, metric))
}
```

```{r}
overview_model <- function(model_cv, model_name) {
  tg <- threshold_graph(model_cv, model_name)
  g1 <- visualize_conf_mat(model_cv, tg$thresholds, "accuracy")
  g2 <- visualize_conf_mat(model_cv, tg$thresholds, "f_meas")
  g3 <- visualize_conf_mat(model_cv, tg$thresholds, "precision")
  
  tg$graph + (g1 / g2 / g3)
}
```

#### --RGB
```{r}
g1 <- overview_model(knn_pgh_cv_results, "KNN in RGB | PCA + hybrid samp.")

# g1 = knn in rgb with pca + hybrid sampling
g1
```
#### optimal threshold KNN in RGB
```{r}
probably::threshold_perf(collect_predictions(knn_pgh_cv_results), BT, .pred_TRUE, 
                         thresholds = 0.87, event_level = "first", 
                         metrics = metric_set(accuracy, precision, f_meas, kap)) |>
  select(c(.metric, .estimate)) |>
  pivot_wider(names_from = ".metric", values_from = ".estimate") |>
  knitr::kable(caption = "<center>Performance metrics for the KNN model at threshold = 0.87", digits =4) %>% 
  kableExtra::kable_styling(full_width = FALSE)
                         
```

#### --EV

```{r}
g2 <- overview_model(knn_pgh_cv_results_ev, "KNN with EV | PCA + hybrid samp.")

g2
```

#### optimal threshold KNN with EV
```{r}
probably::threshold_perf(collect_predictions(knn_pgh_cv_results_ev), BT, .pred_TRUE, 
                         thresholds = 0.56, event_level = "first", 
                         metrics = metric_set(accuracy, precision, f_meas, kap)) |>
  select(c(.metric, .estimate)) |>
  pivot_wider(names_from = ".metric", values_from = ".estimate") |>
  knitr::kable(caption = "<center>Performance metrics for the KNN model at threshold = 0.56", digits =4) %>% 
  kableExtra::kable_styling(full_width = FALSE)
```

### FITTING RF
#### -- RGB
```{r}
# fit tuned model on train
fitmod_rf_phg <- best_wf_rf_pgh %>% 
  fit(data = train_data)
```
#### --EV
```{r}
# fit tuned model on ev-train
fitmod_rf_phg_ev <- best_wf_rf_pgh_ev %>% 
  fit(data = train_engineered)
```

### RF: THRESHOLD OPTIMIZATION
#### -- RGB
```{r}
# RGB
g3 <- overview_model(rf_pgh_cv_results, "RF in RGB")
g3
```

#### optimal threshold RF in RBG
```{r}
probably::threshold_perf(collect_predictions(rf_pgh_cv_results_ev), BT, .pred_TRUE, 
                         thresholds = 0.82, event_level = "first", 
                         metrics = metric_set(accuracy, precision, f_meas, kap)) |>
  select(c(.metric, .estimate)) |>
  pivot_wider(names_from = ".metric", values_from = ".estimate") |>
  knitr::kable(caption = "<center>Performance metrics for the KNN model in RBG at threshold = 0.82", digits =5) %>% 
  kableExtra::kable_styling(full_width = FALSE)
                         
```

#### -- EV
```{r}
# RGB
g4 <- overview_model(rf_pgh_cv_results_ev, "RF on EV")
g4
```

#### optimal threshold RF with EV
```{r}
probably::threshold_perf(collect_predictions(rf_pgh_cv_results_ev), BT, .pred_TRUE, 
                         thresholds = 0.79, event_level = "first", 
                         metrics = metric_set(accuracy, precision, f_meas, kap)) |>
  select(c(.metric, .estimate)) |>
  pivot_wider(names_from = ".metric", values_from = ".estimate") |>
  knitr::kable(caption = "<center>Performance metrics for the KNN model with EV at threshold = 0.79", digits =5) %>% 
  kableExtra::kable_styling(full_width = FALSE)
                         
```

## Performance TESTING
```{r}
# Predict classes at a given threshold
predict_at_threshold <- function(model, data, threshold) {
  model %>%
    augment(data) %>%
    mutate(.pred_class = make_two_class_pred(.pred_TRUE, c("TRUE", "FALSE"), threshold = threshold))
}

# Calculate accuracy and AUC for training and holdout sets
calculate_metrics_at_threshold <- function(model, train, holdout, model_name, threshold) {
  bind_rows(
    # Accuracy on training set
    bind_cols(
      tibble(model = model_name, dataset = "train", threshold = threshold),
      metrics(
        predict_at_threshold(model, train, threshold),
        truth = BT,
        estimate = .pred_class
      )
    ),
    
    # AUC on training set
    bind_cols(
      tibble(model = model_name, dataset = "train", threshold = threshold),
      roc_auc(
        augment(model, new_data = train),
        truth = BT,
        .pred_TRUE,
        event_level = "first"
      )
    ),
    # F1 score on training set
    bind_cols(
      tibble(model = model_name, dataset = "train", threshold = threshold),
      f_meas(
        predict_at_threshold(model, train, threshold),
        truth = BT, 
        estimate = .pred_class, 
        event_level = "first"
      )
    ),
    
    # Accuracy on holdout set
    bind_cols(
      tibble(model = model_name, dataset = "holdout", threshold = threshold),
      metrics(
        predict_at_threshold(model, holdout, threshold),
        truth = BT,
        estimate = .pred_class
      )
    ),
    
    # AUC on holdout set
    bind_cols(
      tibble(model = model_name, dataset = "holdout", threshold = threshold),
      roc_auc(
        augment(model, new_data = holdout),
        truth = BT,
        .pred_TRUE,
        event_level = "first"
      )
    ),
        # F1 score on holdout set
    bind_cols(
      tibble(model = model_name, dataset = "holdout", threshold = threshold),
      f_meas(
        predict_at_threshold(model, holdout, threshold),
        truth = BT, 
        estimate = .pred_class, 
        event_level = "first"
    ),
  )
)
}
```

### Renaming Models
```{r}
# fit models(renaming)
knn_rgb <- fitmod_knn_phg
knn_ev <- fitmod_knn_phg_ev

rf_rgb <- fitmod_rf_phg
rf_ev <- fitmod_rf_phg_ev
```

#### opti thresh
```{r}
# assigning optimal thresholds KNN
opt_thresh_knn_rgb <- 0.87
opt_thresh_knn_ev <- 0.56
# assigning optimal thresholds for RF
opt_thresh_rf_rgb <- 0.82
opt_thresh_rf_ev <- 0.79
```

```{r}
# calculating final performance metrics for KNN
knn_metrics_rgb <- calculate_metrics_at_threshold(knn_rgb, train_data, strat_holdout_sample, "KNN in RGB (PCA + hybrid)", opt_thresh_knn_rgb)
knn_metrics_ev <- calculate_metrics_at_threshold(knn_ev, train_engineered, holdout_engineered, "KNN with EV (PCA + hybrid)", opt_thresh_knn_ev)
```

```{r}
knn_metrics_rgb
```

```{r}
# calculating final performance metrics for RF 
rf_metrics_rgb <- calculate_metrics_at_threshold(rf_rgb, train_data, strat_holdout_sample, "RF in RGB (PCA + hybrid)", opt_thresh_rf_rgb)
rf_metrics_ev <- calculate_metrics_at_threshold(rf_ev, train_engineered, holdout_engineered, "RF with EV (PCA + hybrid)", opt_thresh_rf_ev)
```

### FULL RESULTS TABLE
```{r}
metrics_table <- function(all_metrics, caption) {
  all_metrics %>% 
    pivot_wider(names_from=.metric, values_from = .estimate) %>% 
    select(-.estimator) %>% 
    knitr::kable(caption=caption, digits=5)
}

metrics_at_threshold <- bind_rows(
  knn_metrics_rgb, 
  knn_metrics_ev,
  rf_metrics_rgb,
  rf_metrics_ev
) %>% arrange(dataset)

tuned_metrics_table <- metrics_table(metrics_at_threshold, "<center>Performance Metrics at Optimal Thresholds (optimized for train)") %>% 
  kableExtra::kable_styling(full_width = FALSE)%>%
  kableExtra::collapse_rows(columns = 1:2, valign = "top")

tuned_metrics_table

```

```{r}
#install.packages("magick")
library(magick)
save_kable(tuned_metrics_table, file = "tuned_metrics_table_knn_rf_comb.png", zoom = 2)
```

### KNN-only RESULTS TABLE 
```{r}
# only KNN Table
knn_metrics_at_threshold <- bind_rows(
  knn_metrics_rgb, 
  knn_metrics_ev
) %>% arrange(dataset)

tuned_knn_results_va <- metrics_table(knn_metrics_at_threshold, "<center>Performance Metrics at Optimal Thresholds for Tuned KNN with PCA and hybrid sampling") %>% 
  kableExtra::kable_styling(full_width = FALSE)%>%
  kableExtra::collapse_rows(columns = 1:2, valign = "top")

tuned_knn_results_va

save_kable(tuned_knn_results_va, file = "tuned_knn_results_va.png", zoom = 2)
```

### CONFUSION MATRIX RESULTS KNN
#### -- RGB
```{r}
create_confusion_matrix <- function(model, data, threshold, title = NULL) {
  predictions <- predict_at_threshold(model, data, threshold)
  conf_mat <- conf_mat(predictions, truth = BT, estimate = .pred_class)
  plot <- conf_mat %>%
    autoplot(type = "heatmap") +
    ggtitle(title)
  list(conf_mat = conf_mat, plot = plot)
}
# For training data
# KNN in RGB
cm_knn_rgb_train <- create_confusion_matrix(knn_rgb, train_data, 0.87, "KNN in RGB (PCA +) at Threshold 0.87 - Training")
print(cm_knn_rgb_train$conf_mat)  
cm_knn_rgb_train$plot             

# For 0.2 strat random sampke f holdout data
cm_knn_ev_holdsamp <- create_confusion_matrix(knn_ev, strat_holdout_sample, 0.56, "KNN in EV (PCA +) at Threshold 0.56 - 0.2 Holdout")
print(cm_knn_ev_holdsamp$conf_mat)
cm_knn_ev_holdsamp$plot
```


### RF RESULTS TABLE

#### RF results with PCA
```{r}
# only RF Table
rf_metrics_at_threshold <- bind_rows(
  rf_metrics_rgb,
  rf_metrics_ev
) %>% arrange(dataset)

tuned_rf_results_va <- metrics_table(rf_metrics_at_threshold, "<center>Performance Metrics at Optimal Thresholds (calculated on train) for Tuned Random Forest with PCA and hybrid sampling") %>% 
  kableExtra::kable_styling(full_width = FALSE)%>%
  kableExtra::collapse_rows(columns = 1:2, valign = "top")

save_kable(tuned_rf_results_va, file = "tuned_rf_results_va.png", zoom = 2)
tuned_rf_results_va
```

### CONFUSION MATRIX RESULTS RF

#### RF results with PCA
# with PCA
```{r}
# Function to create a confusion matrix for a model
create_confusion_matrix <- function(model, data, threshold, title = NULL) {
  # Get predictions at the specified threshold
  predictions <- predict_at_threshold(model, data, threshold)
  
  # Create the confusion matrix
  conf_mat <- conf_mat(predictions, truth = BT, estimate = .pred_class)
  
  # Plot with ggplot2 (optional but nice visualization)
  plot <- conf_mat %>%
    autoplot(type = "heatmap") +
    ggtitle(title)
  
  # Return both the confusion matrix object and the plot
  list(conf_mat = conf_mat, plot = plot)
}
################################################################################
# For training data
# RF in RGB
cm_rf_rgb_train <- create_confusion_matrix(rf_rgb, train_data, 0.82, "Random Forest in RGB (PCA +) at Threshold 0.82 - Training")
print(cm_rf_rgb_train$conf_mat)  # Prints the confusion matrix counts
cm_rf_rgb_train$plot             # Displays the plot

# For 0.2 strat random sampke f holdout data
cm_rf_rgb_holdsamp <- create_confusion_matrix(rf_rgb, strat_holdout_sample, 0.82, "Random Forest in RGB (PCA +) at Threshold 0.82 - 0.2 Holdout")
print(cm_rf_rgb_holdsamp$conf_mat)
cm_rf_rgb_holdsamp$plot

# Claude Sonnet helped with this code
```

```{r}
# For full holdout data
cm_rf_rgb_holdfull <- create_confusion_matrix(rf_rgb, holdout_engineered, 0.82, "Random Forest in RGB (PCA +) at Opt. Thresh. (0.82) - Holdout")
print(cm_rf_rgb_holdfull$conf_mat)
cm_rf_rgb_holdfull$plot
```

```{r}
stopCluster(cl)
registerDoSEQ()
```
