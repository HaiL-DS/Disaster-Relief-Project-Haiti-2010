---
title: "Disaster Relief Project - Part 2"
author: "Hai Liu"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
---

```{r setup}
knitr::opts_chunk$set(
  echo = FALSE,
  cache = TRUE,
  autodep = TRUE,
  fig.align = "center",
  fig.pos = "H",
  out.width = "100%"
)
```

```{r echo true, eval=FALSE}
# Set eval to TRUE if you want to see the R code outputs
knitr::opts_chunk$set(
  echo = TRUE)
```


## Parallel Processing Setup and Package Loading
```{r parallel}
#| cache: FALSE
#| message: FALSE
library(doParallel)
cl <- makePSOCKcluster(parallel::detectCores(logical = FALSE) - 2)
registerDoParallel(cl)
```

```{r libraries}
#| cache: FALSE
#| warning: FALSE
#| message: FALSE
library(tidyverse)
library(tidymodels)
library(discrim)
library(jpeg)
library(patchwork)
library(probably)
library(gridExtra)
library(plotly)
library(kableExtra)
library(themis)
```


## Data loading and wrangling
Since we are only interested in the level of "Blue Tarp", I create a new variable `BT` with only two classes, i.e., "TRUE" for "Blue Tarp" and "FALSE" for everything else.

```{r holdout data processing}
# #| message: FALSE
# #| warning: FALSE
# 
# col_names <- c('ID','X','Y','Map X','Map Y','Lat','Lon','Red','Green','Blue')
# 
# blue_files <- c(
#   "HoldOutData/orthovnir069_ROI_Blue_Tarps.txt",
#   "HoldOutData/orthovnir067_ROI_Blue_Tarps.txt",
#   "HoldOutData/orthovnir078_ROI_Blue_Tarps.txt"
# )
# 
# non_blue_files <- c(
#   "HoldOutData/orthovnir057_ROI_NON_Blue_Tarps.txt",
#   "HoldOutData/orthovnir078_ROI_NON_Blue_Tarps.txt",
#   "HoldOutData/orthovnir067_ROI_NOT_Blue_Tarps.txt",
#   "HoldOutData/orthovnir069_ROI_NOT_Blue_Tarps.txt"
# )
# 
# blue_data <- map_dfr(blue_files, ~ 
#   read_table(.x, comment = ";", col_names = col_names, col_types = cols(
#     `Map X` = col_double(),
#     `Map Y` = col_double(),
#     Red = col_integer(),
#     Green = col_integer(),
#     Blue = col_integer()
#   )) %>% 
#     select(`Map X`, `Map Y`, Red, Green, Blue) %>% 
#     mutate(BT = "TRUE")
# )
# 
# non_blue_data <- map_dfr(non_blue_files, ~ 
#   read_table(.x, comment = ";", col_names = col_names, col_types = cols(
#     `Map X` = col_double(),
#     `Map Y` = col_double(),
#     Red = col_integer(),
#     Green = col_integer(),
#     Blue = col_integer()
#   )) %>% 
#     select(`Map X`, `Map Y`, Red, Green, Blue) %>% 
#     mutate(BT = "FALSE")
# )
# 
# holdout_data <- bind_rows(blue_data, non_blue_data) %>% 
#   mutate(BT = factor(BT, levels = c("TRUE", "FALSE")))
```

```{r training data processing}
#| message: FALSE
# 
# train_data <- read_csv("HaitiPixels.csv") %>%
#   mutate(BT = factor(if_else(Class == "Blue Tarp", "TRUE", "FALSE"), levels = c("TRUE", "FALSE"))) %>%
#   select(Red, Green, Blue, BT)
```

```{r}
# Use Clay Harris' data sets with all engineered features features in different color spaces

train_data <- readRDS("./train_data.rds")
holdout_data <- readRDS("./holdout_data.rds") %>% drop_na()
 
```


## Settings for Modelling
```{r}
set.seed(6030)

resamples <- vfold_cv(train_data, v=10, strata=BT)
cv_metrics <- metric_set(accuracy, precision, f_meas, roc_auc)
cv_control <- control_resamples(save_pred=TRUE)
```


## Build and tune the k-nearest neighbors
```{r}
#| warning: FALSE

# RGB Model Formula and Recipe
rgb_formula <- BT ~ Red + Green + Blue
rgb_recipe <- recipe(rgb_formula, data = train_data)

knn_rgb_wf <- workflow() %>%
    add_recipe(rgb_recipe) %>%
    add_model(nearest_neighbor(engine="kknn",
                               mode="classification",
                               neighbors=tune()))
```

```{r}
# parameters <- extract_parameter_set_dials(knn_rgb_wf) %>%
#   update(neighbors = neighbors(c(1, 200)))
# 
# tune_knn_rgb <- tune_grid(knn_rgb_wf,
#                       resamples=resamples,
#                       control=cv_control,
#                       grid=grid_regular(parameters, levels=50))
# 
# autoplot(tune_knn_rgb)

```


```{r}
# tune_knn_rgb2 <- tune_bayes(knn_rgb_wf, 
#                             resamples=resamples, 
#                             param_info=parameters, 
#                             iter=25)
# 
# autoplot(tune_knn_rgb2)
```

```{r}
# parameters <- extract_parameter_set_dials(knn_rgb_wf) %>%
#   update(neighbors = neighbors(c(25, 135)))
# 
# tune_knn_rgb3 <- tune_grid(knn_rgb_wf,
#                       resamples=resamples,
#                       control=cv_control,
#                       grid=grid_regular(parameters, levels=50))
# 
# autoplot(tune_knn_rgb3)
```

```{r}
# tune_knn_rgb4 <- tune_bayes(knn_rgb_wf, 
#                             resamples=resamples, 
#                             param_info=parameters, 
#                             iter=25)
# 
# autoplot(tune_knn_rgb4)
```

```{r}
# show_best(tune_knn_rgb3, metric="roc_auc", n=3)
```
```{r}
# show_best(tune_knn_rgb4, metric="roc_auc", n=3)
```

```{r}
# parameters <- extract_parameter_set_dials(knn_rgb_wf) %>%
#   update(neighbors = neighbors(c(30, 50)))
# 
# tune_knn_rgb5 <- tune_grid(knn_rgb_wf,
#                       resamples=resamples,
#                       control=cv_control,
#                       grid=grid_regular(parameters, levels=20))
# 
# autoplot(tune_knn_rgb5)
```
```{r}
# show_best(tune_knn_rgb5, metric="roc_auc", n=3)
```


## Evaluate the tuned k-nearest neighbor model using 10-fold cross validation.
```{r}
# best_knn_rgb_cv <- knn_rgb_wf %>%
#   finalize_workflow(select_best(tune_knn_rgb5, metric="roc_auc")) %>%
#   fit_resamples(resamples, metrics=cv_metrics, control=cv_control)
```

```{r}
# collect_metrics(best_knn_rgb_cv) %>% mutate(model="KNN")%>%
#   select(model, .metric, mean) %>%
#   pivot_wider(names_from = .metric, values_from = mean) %>%
#   knitr::kable(caption="Cross-validation metrics for KNN RGB model", digits=3)
```

```{r}
# collect_predictions(best_knn_rgb_cv) %>% mutate(model = "KNN") %>%
#   roc_curve(truth = BT, .pred_TRUE, event_level = "first") %>%
#   autoplot() +
#   ggtitle("ROC Curve - KNN RGB Model with 10-Fold CV")
```

## Threshold Selection and Optimization with CV
```{r}
threshold_graph_from_cv <- function(model_cv, model_name) {
    performance <- probably::threshold_perf(collect_predictions(model_cv), 
                                            truth=BT, 
                                            estimate=.pred_TRUE,
                                            thresholds=seq(0.01, 0.99, 0.01), 
                                            event_level="first",
                                            metrics=metric_set(f_meas, accuracy, sens)
                                            )
    
    max_metrics <- performance %>%
        drop_na() %>%
        group_by(.metric) %>%
        filter(.estimate == max(.estimate))
    
    opti_thresholds <- max_metrics %>%
        select(.metric, .threshold) %>%
        deframe()
    
    graph <- ggplot(performance, aes(x=.threshold, y=.estimate, color=.metric)) +
        geom_line() +
        geom_point(data=max_metrics, color="black") +
        labs(title=model_name, x="Threshold", y="Metric value") +
        coord_cartesian(ylim=c(0, 1))
    
    return(list(opti_thresholds=opti_thresholds, graph=graph))
}

visualize_conf_mat <- function(model_cv, opti_thresholds, metric) {
    threshold <- opti_thresholds[metric]
    cm <- collect_predictions(model_cv) %>%
        mutate(
            .pred_class = make_two_class_pred(.pred_TRUE, c("TRUE", "FALSE"), threshold=threshold),
        ) %>%
        conf_mat(truth=BT, estimate=.pred_class)
    autoplot(cm, type="heatmap") +
        labs(title=sprintf("Threshold %.2f (%s)", threshold, metric))
}

overview_model <- function(model_cv, model_name) {
    tg <- threshold_graph_from_cv(model_cv, model_name)
    g1 <- visualize_conf_mat(model_cv, tg$opti_thresholds, "accuracy")
    g2 <- visualize_conf_mat(model_cv, tg$opti_thresholds, "f_meas")
    g3 <- visualize_conf_mat(model_cv, tg$opti_thresholds, "sens")
    tg$graph + (g1 / g2 / g3)
}
```

```{r}
# overview_model(best_knn_rgb_cv, "KNN")
```

## Fit a Final KNN Model and Evaluate with the Holdout Set
```{r}
# final_knn_rgb <- knn_rgb_wf %>%
#   finalize_workflow(select_best(tune_knn_rgb5, metric="roc_auc")) %>%
#   fit(data = train_data)
# ```
# ```{r}
# # Tuned model CV predictions
# cv_preds <- collect_predictions(best_knn_rgb_cv) %>% 
#   mutate(source = "training_CV")
# 
# # Final model prediction
# holdout_preds <- augment(final_knn_rgb, new_data = holdout_data) %>% 
#     mutate(source = "holdout")
#   
# # Compute ROC
# roc_data <- bind_rows(cv_preds, holdout_preds) %>%
#     group_by(source) %>%
#     roc_curve(truth = BT, .pred_TRUE, event_level = "first")
# ```
# ```{r}
# # Use autoplot
# autoplot(roc_data) +
#   ggtitle("ROC Curve - KNN RGB Models") +
#   theme_minimal()
```


## Threshold Selection for Final Models with Holdout Set
```{r}
# Find the optimal threshold
threshold_graph_final_model <- function(final_model, data, model_name) {
    performance <- probably::threshold_perf(augment(final_model, data),
                                            truth = BT,
                                            estimate = .pred_TRUE,
                                            thresholds = seq(0.01, 0.99, 0.01),
                                            event_level = "first",
                                            metrics = metric_set(f_meas)
                                           )
    
    max_metrics <- performance %>%
        drop_na() %>%
        group_by(.metric) %>%
        filter(.estimate == max(.estimate))
    
    opt_thresholds <- max_metrics %>%
        select(.metric, .threshold) %>%
        deframe()
    
    graph <- ggplot(performance, aes(x=.threshold, y=.estimate, color=.metric)) +
        geom_line() +
        geom_point(data=max_metrics, color="black") +
        labs(title=model_name, x="Threshold", y="Metric value") +
        coord_cartesian(ylim=c(0, 1))
    
    return(list(graph=graph, 
                opt_thresholds=opt_thresholds, 
                model_name=model_name))
}
```

```{r}
# final_knn_rgb_threshold_holdout <- threshold_graph_final_model(final_knn_rgb, 
#                                                                holdout_data, 
#                                                                "k-nearest neighbor")
# final_knn_rgb_threshold_holdout$graph
```

```{r}
# final_knn_rgb_threshold_holdout$opt_thresholds['f_meas']
```


## Calculate Metrics at Optimal Thresholds
```{r}
# Two helper functions
predict_at_threshold <- function(model, data, threshold) {
  augment(model, data) %>%
    mutate(
      .pred_class = make_two_class_pred(.pred_TRUE, c("TRUE", "FALSE"), threshold = threshold)
    )
}

calculate_metrics_at_threshold <- function(
  model,
  train,
  holdout,
  model_name,
  color_space,
  train_threshold,
  holdout_threshold
) {
  
  preds_at_thd_train <- predict_at_threshold(model, train, train_threshold)
  preds_at_thd_holdout <- predict_at_threshold(model, holdout, holdout_threshold)

  bind_rows(
    # Metrics for the training set
    bind_cols(
      color_space = color_space,
      model = model_name,
      dataset = "train",
      threshold = train_threshold,
      metrics(preds_at_thd_train, truth = BT, estimate = .pred_class)
    ),
    # bind_cols(
    #   color_space = color_space,
    #   model = model_name,
    #   dataset = "train",
    #   threshold = train_threshold,
    #   sens(preds_at_thd_train, truth = BT, estimate = .pred_class)
    # ),
    bind_cols(
      color_space = color_space,
      model = model_name,
      dataset = "train",
      threshold = train_threshold,
      f_meas(preds_at_thd_train, truth = BT, estimate = .pred_class)
    ),
    bind_cols(
      color_space = color_space,
      model = model_name,
      dataset = "train",
      threshold = train_threshold,
      roc_auc(augment(model, train), truth = BT, .pred_TRUE, event_level = "first")
    ),
    
    
    # Metrics for the holdout set
    bind_cols(
      color_space = color_space,
      model = model_name,
      dataset = "holdout",
      threshold = holdout_threshold,
      metrics(preds_at_thd_holdout, truth = BT, estimate = .pred_class)
    ),
    bind_cols(
      color_space = color_space,
      model = model_name,
      dataset = "holdout",
      threshold = holdout_threshold,
      f_meas(preds_at_thd_holdout, truth = BT, estimate = .pred_class)
    ),
    # bind_cols(
    #   color_space = color_space,
    #   model = model_name,
    #   dataset = "holdout",
    #   threshold = holdout_threshold,
    #   sens(preds_at_thd_holdout, truth = BT, estimate = .pred_class)
    # ),
    bind_cols(
      color_space = color_space,
      model = model_name,
      dataset = "holdout",
      threshold = holdout_threshold,
      roc_auc(augment(model, holdout), BT, .pred_TRUE, event_level = "first")
    )
  )
}
```

```{r}
# calculate_metrics_at_threshold(
#     final_knn_rgb, train_data, holdout_data,
#     "k-nearest neighbor", "RGB",
#     0.58, 0.59) %>%
#   select(-.estimator) %>%
#   pivot_wider(names_from = .metric, values_from = .estimate) %>%
#   knitr::kable(caption="Performance metrics for tuned KNN model with optimal threshold", digits=3)
```


## Downsampling the train_data with SMOTE oversampling to address the unbalanced classes of response variable
```{r}
rgb_hybrid_recipe <- recipe(rgb_formula, data = train_data) %>%
  step_downsample(BT, under_ratio = 6, seed = 6020) %>%  # Down-sample majority class, keeps 1 minority for every 6 majority
  step_smote(BT, over_ratio = 0.33)   # Apply SMOTE to minority class

knn_rgb_hybrid_wf <- workflow() %>%
    add_recipe(rgb_hybrid_recipe) %>%
    add_model(nearest_neighbor(engine="kknn", 
                               mode="classification", 
                               neighbors=tune()))
```


```{r}
# Initial tuning from 1 to 200
# parameters <- extract_parameter_set_dials(knn_rgb_hybrid_wf) %>%
#   update(neighbors = neighbors(c(1, 200)))
# 
# tune_knn_rgb_hybrid <- tune_grid(knn_rgb_hybrid_wf,
#                       resamples=resamples,
#                       control=cv_control,
#                       grid=grid_regular(parameters, levels=50))

```

```{r}
# saving the model
# saveRDS(tune_knn_rgb_hybrid, file = "./tune_knn_rgb_hybrid.rda")
```

```{r}
# tune_knn_rgb_hybrid <- readRDS("./tune_knn_rgb_hybrid.rda")
# autoplot(tune_knn_rgb_hybrid)
```

```{r}
# Second tuning from 20 to 150
# parameters <- extract_parameter_set_dials(knn_rgb_hybrid_wf) %>%
#   update(neighbors = neighbors(c(20, 150)))
# 
# tune_knn_rgb_hybrid2 <- tune_grid(knn_rgb_hybrid_wf,
#                       resamples=resamples,
#                       control=cv_control,
#                       grid=grid_regular(parameters, levels=50))
# 
# autoplot(tune_knn_rgb_hybrid2)
```

```{r}
# saving the model
# saveRDS(tune_knn_rgb_hybrid2, file = "./tune_knn_rgb_hybrid2.rda")
```

```{r}
# tune_knn_rgb_hybrid2 <- readRDS("./tune_knn_rgb_hybrid2.rda")
# autoplot(tune_knn_rgb_hybrid2)
```

```{r}
# show_best(tune_knn_rgb_hybrid2, metric="roc_auc", n=5)
```

```{r}
# Final tuning from 80 to 120
# parameters <- extract_parameter_set_dials(knn_rgb_hybrid_wf) %>%
#   update(neighbors = neighbors(c(80, 120)))
# 
# tune_knn_rgb_hybrid3 <- tune_grid(knn_rgb_hybrid_wf,
#                       resamples=resamples,
#                       control=cv_control,
#                       grid=grid_regular(parameters, levels=40))

```

```{r}
# saving the model
# saveRDS(tune_knn_rgb_hybrid3, file = "./tune_knn_rgb_hybrid3.rda")
```

```{r}
# tune_knn_rgb_hybrid3 <- readRDS("./tune_knn_rgb_hybrid3.rda")
# autoplot(tune_knn_rgb_hybrid3)
```

```{r}
# show_best(tune_knn_rgb_hybrid3, metric="roc_auc", n=3)
```


## Evaluate the down-sampled tuned k-nearest neighbor model using 10-fold cross validation.
```{r}
# # Fit back to original workflow without down-sampling or SMOTE
# best_knn_rgb_hybrid_cv <- knn_rgb_wf %>%
#   finalize_workflow(select_best(tune_knn_rgb_hybrid3, metric="roc_auc")) %>%
#   fit_resamples(resamples, metrics=cv_metrics, control=cv_control)
# 
# # saving the model
# saveRDS(best_knn_rgb_hybrid_cv, file = "./best_knn_rgb_hybrid_cv.rda")
```

```{r}
# best_knn_rgb_hybrid_cv <- readRDS("./best_knn_rgb_hybrid_cv.rda")
# 
# collect_metrics(best_knn_rgb_hybrid_cv) %>% mutate(model="Tuned KNN DS&SMOTE Model")%>%
#   select(model, .metric, mean) %>%
#   pivot_wider(names_from = .metric, values_from = mean) %>%
#   knitr::kable(caption="Cross-validation metrics for KNN models by Down-sample&SMOTE", digits=3)
```

```{r}
# collect_predictions(best_knn_rgb_hybrid_cv) %>% mutate(model = "k-nearest neighbor DS&SMOTE") %>%
#   roc_curve(truth = BT, .pred_TRUE, event_level = "first") %>%
#   autoplot() +
#   ggtitle("ROC Curve - KNN RGB Model with DS&SMOTE")
```

```{r}
# overview_model(best_knn_rgb_hybrid_cv, "KNN with DS&SMOTE")
```

```{r}
# # finalizing the model with full training data
# final_knn_rgb_hybrid <- knn_rgb_wf %>%
#   finalize_workflow(select_best(tune_knn_rgb_hybrid3, metric="roc_auc")) %>%
#   fit(data = train_data)
# 
# # saving the model
# saveRDS(final_knn_rgb_hybrid, file = "./final_knn_rgb_hybrid.rda")
```

```{r}
final_knn_rgb_hybrid <- readRDS("./final_knn_rgb_hybrid.rda")
```

```{r}
# # Tuned model CV predictions
# cv_preds <- collect_predictions(best_knn_rgb_hybrid_cv) %>% 
#   mutate(source = "training_CV")
# 
# # Final model prediction
# holdout_preds <- augment(final_knn_rgb_hybrid, new_data = holdout_data) %>%
#   mutate(source = "holdout")
# 
# # Compute ROC
# knn_rgb_hybrid_roc_data <- bind_rows(cv_preds, holdout_preds) %>%
#     group_by(source) %>%
#     roc_curve(truth = BT, .pred_TRUE, event_level = "first")
# 
# saveRDS(knn_rgb_hybrid_roc_data, file = "./knn_rgb_hybrid_roc_data.rda")
```

```{r}
# overlay the ROC curves from CV and holdout data

knn_rgb_hybrid_roc_data <- readRDS("./knn_rgb_hybrid_roc_data.rda")

autoplot(knn_rgb_hybrid_roc_data) +
  ggtitle("k-nearest neighbor after DS&SMOTE") +
  theme_minimal()
```


```{r}
# # Find and plot the optimal threshold for Holdout Set based on f_meas
# final_knn_rgb_hybrid_threshold_holdout <- threshold_graph_final_model(final_knn_rgb_hybrid,
#                                                          holdout_data,
#                                                          "KNN after DS&SMOTE")
# saveRDS(final_knn_rgb_hybrid_threshold_holdout, file = "./final_knn_rgb_hybrid_threshold_holdout.rda")
```

```{r}
# final_knn_rgb_hybrid_threshold_holdout <- readRDS("./final_knn_rgb_hybrid_threshold_holdout.rda")
# final_knn_rgb_hybrid_threshold_holdout$graph
```

```{r}
# final_knn_rgb_hybrid_threshold_holdout$opt_thresholds['f_meas']
```

```{r}
# Calculate Metrics at the Optimal Thresholds
# calculate_metrics_at_threshold(
#     final_knn_rgb_hybrid, train_data, holdout_data,
#     "k-nearest neighbor after DS&SMOTE", "RGB",
#     0.49, 0.64) %>%
#   select(-.estimator) %>%
#   pivot_wider(names_from = .metric, values_from = .estimate) %>%
#   knitr::kable(caption="Performance metrics for tuned KNN model after DS&SMOTE with optimal threshold", digits=3)
```


### Build and Tune a KNN Model with All Engineered Features
```{r}
#| warning: FALSE

# KNN Model Formula, Recipe, and Workflow with All Features
all_formula <- BT ~ Red + Green + Blue + Luminance + a + b + Hue + Saturation + Value + Red_Prop + Green_Prop + Blue_Prop + Dispersion + Hue_Shifted + Red_9 + Green_9 + Blue_9 + Luminance_9 + a_9 + b_9 + Hue_9 + Saturation_9 + Value_9 + Red_Prop_9 + Green_Prop_9 + Blue_Prop_9 + Dispersion_9 + Hue_Shifted_9

all_recipe <- recipe(all_formula, data = train_data) %>%  
  step_normalize(all_numeric_predictors()) %>% 
  step_pca(all_numeric_predictors(), num_comp=tune())

# RUS and SMOTE for Model Tuning
all_hybrid_recipe <- recipe(all_formula, data = train_data) %>%
  step_downsample(BT, under_ratio = 6, seed = 6020) %>%  # RUS majority class, keeps 1 minority for every 6 majority
  step_smote(BT, over_ratio = 0.33) %>%    # Apply SMOTE to minority class
  step_normalize(all_numeric_predictors()) %>% 
  step_pca(all_numeric_predictors(), num_comp=tune())

knn_all_hybrid_wf <- workflow() %>%
    add_recipe(all_hybrid_recipe) %>%
    add_model(nearest_neighbor(engine="kknn", 
                               mode="classification", 
                               neighbors=tune()))

knn_all_wf <- workflow() %>%
    add_recipe(all_recipe) %>%
    add_model(nearest_neighbor(engine="kknn",
                               mode="classification",
                               neighbors=tune()))
```

```{r}
# Initial tuning from 1 to 200
# parameters <- extract_parameter_set_dials(knn_all_hybrid_wf) %>%
#   update(neighbors = neighbors(c(1, 200)))
# 
# tune_knn_all_hybrid1 <- tune_bayes(knn_all_hybrid_wf,
#                       resamples=resamples,
#                       param_info=parameters, iter=25)
# 
# saveRDS(tune_knn_all_hybrid1, file = "./tune_knn_all_hybrid1.rda")
```


```{r}
tune_knn_all_hybrid1 <- readRDS("./tune_knn_all_hybrid1.rda")
autoplot(tune_knn_all_hybrid1)
```

```{r}
show_best(tune_knn_all_hybrid1, metric="roc_auc", n=3)
```


## Evaluate the KNN Model (All Features) with 10-fold CV.
```{r}
# Fit the model with the workflow without RUS&SMOTE
# best_knn_all_cv <- knn_all_wf %>%
#   finalize_workflow(select_best(tune_knn_all_hybrid1, metric="roc_auc")) %>%
#   fit_resamples(resamples, metrics=cv_metrics, control=cv_control)

# saving the model
# saveRDS(best_knn_all_cv, file = "./best_knn_all_cv.rda")
```

```{r}
best_knn_all_cv <- readRDS("./best_knn_all_cv.rda")

collect_metrics(best_knn_all_cv) %>% mutate(model="Tuned KNN with All Predictors")%>%
  select(model, .metric, mean) %>%
  pivot_wider(names_from = .metric, values_from = mean) %>%
  knitr::kable(caption="Cross-validation metrics for KNN model with all predictors", digits=3)
```


## Threshold Selection for the KNN Model (All Features) with 10-fold CV.
```{r}
# overview_model(best_knn_all_cv, "Tuned KNN with All Predictors")
```


## Tuned KNN Model (All Features) Threshold and Performance on Holdout/Test Data
```{r}
# # finalizing the model with full training data
# final_knn_all <- knn_all_wf %>%
#   finalize_workflow(select_best(tune_knn_all_hybrid1, metric="roc_auc")) %>%
#   fit(data = train_data)
# 
# # saving the model
# saveRDS(final_knn_all, file = "./final_knn_all.rda")
```

```{r}
final_knn_all <- readRDS("./final_knn_all.rda")
```

```{r}
# # Tuned model CV predictions
# cv_preds <- collect_predictions(best_knn_all_cv) %>%
#   mutate(source = "training_CV")
# 
# # Final model prediction
# holdout_preds <- augment(final_knn_all, new_data = holdout_data) %>%
#   mutate(source = "holdout")
# 
# # Compute ROC
# knn_all_roc_data <- bind_rows(cv_preds, holdout_preds) %>%
#     group_by(source) %>%
#     roc_curve(truth = BT, .pred_TRUE, event_level = "first")
# 
# saveRDS(knn_all_roc_data, file = "./knn_all_roc_data.rda")
```

```{r}
# Overlay the ROC Curves from CV and Holdout Data for RGB and All-feature Spaces

knn_all_roc_data <- readRDS("./knn_all_roc_data.rda")

knn_rgb_all_roc_data <- bind_rows(knn_rgb_hybrid_roc_data %>% mutate(color_space="RGB"), 
                                  knn_all_roc_data %>% mutate(color_space="All Predictors"))

g_knn_all <- knn_rgb_all_roc_data %>% 
  ggplot(aes(x=1 - specificity, y=sensitivity, color=color_space)) +
    geom_line(aes(linetype = source))+
    ggtitle("Tuned KNN Models with Different Feature Spaces") #+ theme_minimal()

g_knn_all
```

```{r}
g_knn_all_zoom <- g_knn_all +
coord_cartesian(xlim=c(0, 0.2), ylim=c(0.8, 1)) +
guides(colour="none")

g_knn_all_zoom
```


```{r}
# Find and plot the optimal threshold for Holdout Set based on f_meas
# final_knn_all_threshold_holdout <- threshold_graph_final_model(final_knn_all,
#                                                          holdout_data,
#                                                          "KNN All Features")
# saveRDS(final_knn_all_threshold_holdout, file = "./final_knn_all_threshold_holdout.rda")
```

```{r}
final_knn_all_threshold_holdout <- readRDS("./final_knn_all_threshold_holdout.rda")
final_knn_all_threshold_holdout$graph
```

```{r}
final_knn_all_threshold_holdout$opt_thresholds['f_meas']
```

```{r}
# Calculate Metrics at the Optimal Thresholds

# Calculate with full holdout appear to be too resource demanding to run successfully,
# only use a 20% stratified sample of the holdout as recommended by Virginia Brame.
# holdout_stratsample <- holdout_data %>% 
#   na.omit() %>% 
#   group_by(BT) %>% 
#   sample_frac(size = 0.2)
# 
# metrics_knn_all <- calculate_metrics_at_threshold(
#     final_knn_all, train_data, holdout_stratsample,
#     "k-nearest neighbor", "All Predictors", 0.49, 0.99) %>% select(-.estimator) 
# 
# saveRDS(metrics_knn_all, file = "./metrics_knn_all.rda")
```

```{r}
metrics_knn_all = readRDS("./metrics_knn_all.rda")

metrics_knn_all %>%
  pivot_wider(names_from = .metric, values_from = .estimate) %>%
  knitr::kable(caption="Performance metrics for tuned KNN models at optimal thresholds in all feature space", digits=3)

```























## Build and tune Support Vector Machines (SVM)
### SVM - linear kernel (RGB)
```{r}
# #| message: FALSE
# #| warning: FALSE
# 
# svm_rgb_linear_hybrid_wf <- workflow() %>%
#   add_recipe(rgb_hybrid_recipe) %>%
#   add_model(svm_linear(mode="classification", 
#                          engine="kernlab", 
#                          cost=tune(), 
#                          margin=tune()))
# 
# parameters <- extract_parameter_set_dials(svm_rgb_linear_hybrid_wf)
# 
# tune_svm_rgb_linear <- tune_bayes(svm_rgb_linear_hybrid_wf,
#                                   resamples=resamples,
#                                   param_info=parameters, iter=25)
# 
# autoplot(tune_svm_rgb_linear)

```

```{r}
# show_best(tune_svm_rgb_linear, metric="roc_auc", n=3)
```
```{r}
# saving the model
# saveRDS(tune_svm_rgb_linear, file = "./tune_svm_rgb_linear.rda")
```

```{r}
# tune_svm_rgb_linear = readRDS("./tune_svm_rgb_linear.rda")
# show_best(tune_svm_rgb_linear, metric="roc_auc", n=3)
```

### SVM - linear kernel (All Features)
```{r}
#| message: FALSE
#| warning: FALSE

svm_all_linear_hybrid_wf <- workflow() %>%
  add_recipe(all_hybrid_recipe) %>%
  add_model(svm_linear(mode="classification",
                         engine="kernlab",
                         cost=tune(),
                         margin=tune()))

svm_all_linear_wf <- workflow() %>%
    add_recipe(all_recipe) %>%
    add_model(svm_linear(mode="classification",
                         engine="kernlab",
                         cost=tune(),
                         margin=tune()))


# parameters <- extract_parameter_set_dials(svm_all_linear_hybrid_wf)
# 
# tune_svm_all_linear <- tune_bayes(svm_all_linear_hybrid_wf,
#                                   resamples=resamples,
#                                   param_info=parameters, iter=20)
# 
# autoplot(tune_svm_all_linear)
```

```{r}
# show_best(tune_svm_all_linear, metric="roc_auc", n=3)
```


### SVM - polynomial kernel (RGB)
```{r}
# #| message: FALSE
# #| warning: FALSE
# svm_rgb_poly_hybrid_wf <- workflow() %>%
#   add_recipe(rgb_hybrid_recipe) %>%
#   add_model(svm_poly(mode="classification", 
#                        engine="kernlab",
#                        margin=tune(),
#                        degree=tune(), 
#                        cost=tune()))
# 
# parameters <- extract_parameter_set_dials(svm_rgb_poly_hybrid_wf) %>%
#     update(degree = degree_int(range=c(2, 5)))
# 
# tune_svm_rgb_poly <- tune_bayes(svm_rgb_poly_hybrid_wf,
#                                 resamples=resamples,
#                                 param_info=parameters, iter=25)
# 
# autoplot(tune_svm_rgb_poly)
```

```{r}
# show_best(tune_svm_rgb_poly, metric="roc_auc", n=3)
```
```{r}
# saving the model
# saveRDS(tune_svm_rgb_poly, file = "./tune_svm_rgb_poly.rda")
```

```{r}
# tune_svm_rgb_poly = readRDS("./tune_svm_rgb_poly.rda")
# show_best(tune_svm_rgb_poly, metric="roc_auc", n=3)
```


### SVM - polynomial kernel (All Features)
```{r}
#| message: FALSE
#| warning: FALSE
svm_all_poly_hybrid_wf <- workflow() %>%
  add_recipe(all_hybrid_recipe) %>%
  add_model(svm_poly(mode="classification",
                       engine="kernlab",
                       margin=tune(),
                       degree=tune(),
                       cost=tune()))

svm_all_poly_wf <- workflow() %>%
  add_recipe(all_recipe) %>%
  add_model(svm_poly(mode="classification",
                       engine="kernlab",
                       margin=tune(),
                       degree=tune(),
                       cost=tune()))
  

# parameters <- extract_parameter_set_dials(svm_all_poly_hybrid_wf) %>%
#     update(degree = degree_int(range=c(2, 5)))
# 
# tune_svm_all_poly <- tune_bayes(svm_all_poly_hybrid_wf,
#                                 resamples=resamples,
#                                 param_info=parameters, iter=25)
# 
# autoplot(tune_svm_all_poly)
```

```{r}
# show_best(tune_svm_all_poly, metric="roc_auc", n=3)
```


### SVM - radial basis function kernel (RGB)
```{r}
# #| message: FALSE
# #| warning: FALSE
# 
# svm_rgb_rbf_hybrid_wf <- workflow() %>%
#   add_recipe(rgb_hybrid_recipe) %>%
#   add_model(svm_rbf(mode="classification", 
#                       engine="kernlab",
#                       rbf_sigma=tune(), 
#                       cost=tune(), 
#                       margin=tune()))
# 
# parameters <- extract_parameter_set_dials(svm_rgb_rbf_hybrid_wf) %>%
#     update(rbf_sigma = rbf_sigma(range=c(-4, 0), trans=log10_trans()))
# 
# tune_svm_rgb_rbf <- tune_bayes(svm_rgb_rbf_hybrid_wf,
#                                resamples=resamples,
#                                param_info=parameters, iter=25)
# 
# autoplot(tune_svm_rgb_rbf)

```

```{r}
# show_best(tune_svm_rgb_rbf, metric="roc_auc", n=3)
```

```{r}
# saving the model
# saveRDS(tune_svm_rgb_rbf, file = "./tune_svm_rgb_rbf.rda")
```

```{r}
# tune_svm_rgb_rbf = readRDS("./tune_svm_rgb_rbf.rda")
# show_best(tune_svm_rgb_rbf, metric="roc_auc", n=3)
```


### SVM - radial basis function kernel (All Features)
```{r}
#| message: FALSE
#| warning: FALSE

svm_all_rbf_hybrid_wf <- workflow() %>%
  add_recipe(all_hybrid_recipe) %>%
  add_model(svm_rbf(mode="classification", 
                      engine="kernlab",
                      rbf_sigma=tune(),
                      cost=tune(),
                      margin=tune()))

svm_all_rbf_wf <- workflow() %>%
  add_recipe(all_recipe) %>%
  add_model(svm_rbf(mode="classification", 
                      engine="kernlab",
                      rbf_sigma=tune(),
                      cost=tune(),
                      margin=tune()))
  

# parameters <- extract_parameter_set_dials(svm_all_rbf_hybrid_wf) %>%
#     update(rbf_sigma = rbf_sigma(range=c(-4, 0), trans=log10_trans()))
# 
# tune_svm_all_rbf <- tune_bayes(svm_all_rbf_hybrid_wf,
#                                resamples=resamples,
#                                param_info=parameters, iter=25)
# 
# autoplot(tune_svm_all_rbf)
```

```{r}
# show_best(tune_svm_all_rbf, metric="roc_auc", n=3)
```



## Evaluate and Compare Performance of SVM Models with CV
### RGB space
```{r}
# #| message: FALSE
# #| warning: FALSE
# # Fit three best tuned models with cross-validation without DS&SMOTE
# 
# svm_rgb_linear_wf <- workflow() %>%
#   add_recipe(rgb_recipe) %>%
#   add_model(svm_linear(mode="classification",
#                          engine="kernlab",
#                          cost=tune(),
#                          margin=tune()))
# 
# svm_rgb_poly_wf <- workflow() %>%
#   add_recipe(rgb_recipe) %>%
#   add_model(svm_poly(mode="classification",
#                        engine="kernlab",
#                        margin=tune(),
#                        degree=tune(),
#                        cost=tune()))
# 
# svm_rgb_rbf_wf <- workflow() %>%
#   add_recipe(rgb_recipe) %>%
#   add_model(svm_rbf(mode="classification",
#                       engine="kernlab",
#                       rbf_sigma=tune(),
#                       cost=tune(),
#                       margin=tune()))
# 
# best_svm_rgb_linear_cv <- svm_rgb_linear_wf %>%
#   finalize_workflow(select_best(tune_svm_rgb_linear, metric="roc_auc")) %>%
#   fit_resamples(resamples=resamples, metrics=cv_metrics, control=cv_control)
# 
# best_svm_rgb_poly_cv <- svm_rgb_poly_wf %>%
#   finalize_workflow(select_best(tune_svm_rgb_poly, metric="roc_auc")) %>%
#   fit_resamples(resamples=resamples, metrics=cv_metrics, control=cv_control)
# 
# best_svm_rgb_rbf_cv <- svm_rgb_rbf_wf %>%
#   finalize_workflow(select_best(tune_svm_rgb_rbf, metric="roc_auc")) %>%
#   fit_resamples(resamples=resamples, metrics=cv_metrics, control=cv_control)
# 
# saveRDS(best_svm_rgb_linear_cv, file = "./best_svm_rgb_linear_cv.rda")  # save the cv models
# saveRDS(best_svm_rgb_poly_cv, file = "./best_svm_rgb_poly_cv.rda")
# saveRDS(best_svm_rgb_rbf_cv, file = "./best_svm_rgb_rbf_cv.rda")

```

```{r}
best_svm_rgb_linear_cv = readRDS("./best_svm_rgb_linear_cv.rda")
best_svm_rgb_poly_cv = readRDS("./best_svm_rgb_poly_cv.rda")
best_svm_rgb_rbf_cv = readRDS("./best_svm_rgb_rbf_cv.rda")

```


```{r}
# Overlaying ROC curves for the three tuned SMV models with CV
roc_cv_data <- function(model_cv) {
  cv_predictions <- collect_predictions(model_cv)
  cv_predictions %>%
    roc_curve(truth = BT, .pred_TRUE, event_level = "first")
  }

svm_rgb_roc_cv <- bind_rows(
  roc_cv_data(best_svm_rgb_linear_cv) %>% 
    mutate(Tuned_model="SVM with linear kernel", Color_space="RGB"),
  roc_cv_data(best_svm_rgb_poly_cv) %>% 
    mutate(Tuned_model="SVM with polynomial kernel", Color_space="RGB"),
  roc_cv_data(best_svm_rgb_rbf_cv) %>% 
    mutate(Tuned_model="SVM with radial basis function kernel", Color_space="RGB")
  ) 

g <- svm_rgb_roc_cv %>%
  ggplot(aes(x=1 - specificity, y=sensitivity, color=Tuned_model)) +
  geom_line() +
  theme(aspect.ratio=1)
g
```

```{r}
g_zoom <- g +
coord_cartesian(xlim=c(0, 0.2), ylim=c(0.8, 1)) +
guides(colour="none")

g_zoom
```

```{r}
# Compare performance metrics

bind_rows(
  collect_metrics(best_svm_rgb_linear_cv) %>% mutate(model="SVM with linear kernel"),
  collect_metrics(best_svm_rgb_poly_cv) %>% mutate(model="SVM with polynomial kernel"),
  collect_metrics(best_svm_rgb_rbf_cv) %>% mutate(model="SVM with radial basis function kernel")
  ) %>%
  select(model, .metric, mean) %>%
  pivot_wider(names_from = .metric, values_from = mean) %>%
  knitr::kable(caption="Cross-validation performance metrics", digits=3)
```


### All feature space
```{r}
#| message: FALSE
#| warning: FALSE
# Fit three best tuned models with cross-validation without DS&SMOTE

# best_svm_all_linear_cv <- svm_all_linear_wf %>%
#   finalize_workflow(select_best(tune_svm_all_linear, metric="roc_auc")) %>%
#   fit_resamples(resamples=resamples, metrics=cv_metrics, control=cv_control)
# 
# best_svm_all_poly_cv <- svm_all_poly_wf %>%
#   finalize_workflow(select_best(tune_svm_all_poly, metric="roc_auc")) %>%
#   fit_resamples(resamples=resamples, metrics=cv_metrics, control=cv_control)
# 
# best_svm_all_rbf_cv <- svm_all_rbf_wf %>%
#   finalize_workflow(select_best(tune_svm_all_rbf, metric="roc_auc")) %>%
#   fit_resamples(resamples=resamples, metrics=cv_metrics, control=cv_control)
# 
# saveRDS(best_svm_all_linear_cv, file = "./best_svm_all_linear_cv.rda")  # save the cv models
# saveRDS(best_svm_all_poly_cv, file = "./best_svm_all_poly_cv.rda")
# saveRDS(best_svm_all_rbf_cv, file = "./best_svm_all_rbf_cv.rda")

```

```{r}
best_svm_all_linear_cv = readRDS("./best_svm_all_linear_cv.rda")
best_svm_all_poly_cv = readRDS("./best_svm_all_poly_cv.rda")
best_svm_all_rbf_cv = readRDS("./best_svm_all_rbf_cv.rda")

```


```{r}
# Overlaying ROC curves for the three tuned SMV models with CV

svm_roc_cv <- bind_rows(
  roc_cv_data(best_svm_all_linear_cv) %>% 
    mutate(Tuned_model="SVM with linear kernel", Color_space="All Predictors"),
  roc_cv_data(best_svm_all_poly_cv) %>% 
    mutate(Tuned_model="SVM with polynomial kernel", Color_space="All Predictors"),
  roc_cv_data(best_svm_all_rbf_cv) %>% 
    mutate(Tuned_model="SVM with radial basis function kernel", Color_space="All Predictors"),
  svm_rgb_roc_cv
  )

g_all <- svm_roc_cv %>%
  ggplot(aes(x=1 - specificity, y=sensitivity, color=Tuned_model)) +
  geom_line(aes(linetype = Color_space)) +
  theme(aspect.ratio=1)

g_all
```

```{r}
g_all_zoom <- g_all +
coord_cartesian(xlim=c(0, 0.2), ylim=c(0.8, 1)) +
guides(colour="none")

g_all_zoom
```

```{r}
# Compare CV performance metrics (All feature space)

bind_rows(
  collect_metrics(best_svm_all_linear_cv) %>% mutate(model="SVM with linear kernel"),
  collect_metrics(best_svm_all_poly_cv) %>% mutate(model="SVM with polynomial kernel"),
  collect_metrics(best_svm_all_rbf_cv) %>% mutate(model="SVM with radial basis function kernel")
  ) %>%
  select(model, .metric, mean) %>%
  pivot_wider(names_from = .metric, values_from = mean) %>%
  knitr::kable(caption="Cross-validation performance metrics for SVM models with all feature space", digits=3)
```


### Threshold Selection with CV for the three tuned SMV models (RGB)
```{r}
# Threshold Selection with CV for the three tuned SMV models
# overview_model(best_svm_rgb_linear_cv, "SVM with linear kernel")
# overview_model(best_svm_rgb_poly_cv, "SVM with polynomial kernel")
# overview_model(best_svm_rgb_rbf_cv, "SVM with radial basis function kernel")

```

### Threshold Selection with CV for the three tuned SMV models (All features)
```{r}
# overview_model(best_svm_all_linear_cv, "SVM with linear kernel")
# overview_model(best_svm_all_poly_cv, "SVM with polynomial kernel")
# overview_model(best_svm_all_rbf_cv, "SVM with radial basis function kernel")
```


### Finalize the SVM Models with Full Training Set and Threshold Selection for the Holdout Set (RGB)
```{r}
# Finalize all three SVM models
# final_svm_rgb_linear <- svm_rgb_linear_wf %>%
#   finalize_workflow(select_best(tune_svm_rgb_linear, metric="roc_auc")) %>%
#   fit(data = train_data)
# 
# final_svm_rgb_poly <- svm_rgb_poly_wf %>%
#   finalize_workflow(select_best(tune_svm_rgb_poly, metric="roc_auc")) %>%
#   fit(data = train_data)
# 
# final_svm_rgb_rbf <- svm_rgb_rbf_wf %>%
#   finalize_workflow(select_best(tune_svm_rgb_rbf, metric="roc_auc")) %>%
#   fit(data = train_data)
# 
# saveRDS(final_svm_rgb_linear, file = "./final_svm_rgb_linear.rda")  # save the cv models
# saveRDS(final_svm_rgb_poly, file = "./final_svm_rgb_poly.rda")
# saveRDS(final_svm_rgb_rbf, file = "./final_svm_rgb_rbf.rda")

```

```{r}
final_svm_rgb_linear = readRDS("./final_svm_rgb_linear.rda")
final_svm_rgb_poly = readRDS("./final_svm_rgb_poly.rda")
final_svm_rgb_rbf = readRDS("./final_svm_rgb_rbf.rda")

```


```{r}
# Threshold Selection for Holdout Set
# final_svm_rgb_linear_threshold_holdout <- threshold_graph_final_model(final_svm_rgb_linear,
#                                                     holdout_data,
#                                                     "SVM with linear kernel")
# 
# final_svm_rgb_poly_threshold_holdout <- threshold_graph_final_model(final_svm_rgb_poly,
#                                                     holdout_data,
#                                                     "SVM with polynomial kernel")
# 
# final_svm_rgb_rbf_threshold_holdout <- threshold_graph_final_model(final_svm_rgb_rbf,
#                                                     holdout_data,
#                                                     "SVM with radial basis function kernel")
# 
# final_svm_rgb_linear_threshold_holdout$graph
# final_svm_rgb_poly_threshold_holdout$graph
# final_svm_rgb_rbf_threshold_holdout$graph
```

```{r}
# final_svm_rgb_linear_threshold_holdout$opt_thresholds['f_meas']
```
```{r}
# final_svm_rgb_poly_threshold_holdout$opt_thresholds['f_meas']
```
```{r}
# final_svm_rgb_rbf_threshold_holdout$opt_thresholds['f_meas']
```


### Finalize the SVM Models with Full Training Set and Threshold Selection for the Holdout Set (All features)
```{r}
# Finalize all three SVM models in all feature space
# final_svm_all_linear <- svm_all_linear_wf %>%
#   finalize_workflow(select_best(tune_svm_all_linear, metric="roc_auc")) %>%
#   fit(data = train_data)
# 
# final_svm_all_poly <- svm_all_poly_wf %>%
#   finalize_workflow(select_best(tune_svm_all_poly, metric="roc_auc")) %>%
#   fit(data = train_data)
# 
# final_svm_all_rbf <- svm_all_rbf_wf %>%
#   finalize_workflow(select_best(tune_svm_all_rbf, metric="roc_auc")) %>%
#   fit(data = train_data)
# 
# saveRDS(final_svm_all_linear, file = "./final_svm_all_linear.rda")  # save the cv models
# saveRDS(final_svm_all_poly, file = "./final_svm_all_poly.rda")
# saveRDS(final_svm_all_rbf, file = "./final_svm_all_rbf.rda")
```

```{r}
final_svm_all_linear = readRDS("./final_svm_all_linear.rda")
final_svm_all_poly = readRDS("./final_svm_all_poly.rda")
final_svm_all_rbf = readRDS("./final_svm_all_rbf.rda")

```


```{r}
# Threshold Selection for Holdout Set

# final_svm_all_linear_threshold_holdout <- threshold_graph_final_model(final_svm_all_linear,
#                                                     holdout_data,
#                                                     "SVM with linear kernel")
# 
# final_svm_all_poly_threshold_holdout <- threshold_graph_final_model(final_svm_all_poly,
#                                                     holdout_data,
#                                                     "SVM with polynomial kernel")
# 
# final_svm_all_rbf_threshold_holdout <- threshold_graph_final_model(final_svm_all_rbf,
#                                                     holdout_data,
#                                                     "SVM with radial basis function kernel")
# 
# final_svm_all_linear_threshold_holdout$graph
# final_svm_all_poly_threshold_holdout$graph
# final_svm_all_rbf_threshold_holdout$graph
```

```{r}
# final_svm_all_linear_threshold_holdout$opt_thresholds['f_meas']
```
```{r}
# final_svm_all_poly_threshold_holdout$opt_thresholds['f_meas']
```
```{r}
# final_svm_all_rbf_threshold_holdout$opt_thresholds['f_meas']
```


### Overlay ROC curves for three SVM models between two RGB and All-feature with training and holdout data
```{r}
# For SVM with linear kernel

# Tuned model CV predictions
cv_rgb_preds <- collect_predictions(best_svm_rgb_linear_cv) %>%
  roc_curve(truth = BT, .pred_TRUE, event_level = "first") %>%
  mutate(Source = "training_CV", Color_space = "RGB")

cv_all_preds <- collect_predictions(best_svm_all_linear_cv) %>%
  roc_curve(truth = BT, .pred_TRUE, event_level = "first") %>%
  mutate(Source = "training_CV", Color_space = "All Predictors")

# Final model prediction
holdout_rgb_preds <- augment(final_svm_rgb_linear, new_data = holdout_data) %>%
  roc_curve(truth = BT, .pred_TRUE, event_level = "first") %>%
  mutate(Source = "holdout", Color_space = "RGB")

holdout_all_preds <- augment(final_svm_all_linear, new_data = holdout_data) %>%
  roc_curve(truth = BT, .pred_TRUE, event_level = "first") %>%
  mutate(Source = "holdout", Color_space = "All Predictors")

# Compute ROC
svm_linear_roc_data <- bind_rows(cv_rgb_preds, cv_all_preds, holdout_rgb_preds, holdout_all_preds)

saveRDS(svm_linear_roc_data, file = "./svm_linear_roc_data.rda")
```

```{r}
#svm_linear_roc_data = readRDS("./svm_linear_roc_data.rda")
g_svm_linear <- svm_linear_roc_data %>%
  ggplot(aes(x=1 - specificity, y=sensitivity, color=Color_space)) +
    geom_line(aes(linetype = Source)) +
    theme(aspect.ratio=1)

g_svm_linear
```
```{r}
g_svm_linear_zoom <- g_svm_linear +
coord_cartesian(xlim=c(0, 0.2), ylim=c(0.8, 1)) +
guides(colour="none")

g_svm_linear_zoom
```




```{r}
# For SVM with polynormial kernel

# Tuned model CV predictions
cv_rgb_preds <- collect_predictions(best_svm_rgb_poly_cv) %>%
  roc_curve(truth = BT, .pred_TRUE, event_level = "first") %>%
  mutate(Source = "training_CV", Color_space = "RGB")
cv_all_preds <- collect_predictions(best_svm_all_poly_cv) %>%
  roc_curve(truth = BT, .pred_TRUE, event_level = "first") %>%
  mutate(Source = "training_CV", Color_space = "All Predictors")

# Final model prediction
holdout_rgb_preds <- augment(final_svm_rgb_poly, new_data = holdout_data) %>%
  roc_curve(truth = BT, .pred_TRUE, event_level = "first") %>%
  mutate(Source = "holdout", Color_space = "RGB")
holdout_all_preds <- augment(final_svm_all_poly, new_data = holdout_data) %>%
  roc_curve(truth = BT, .pred_TRUE, event_level = "first") %>%
  mutate(Source = "holdout", Color_space = "All Predictors")

# Compute ROC
svm_poly_roc_data <- bind_rows(cv_rgb_preds, cv_all_preds, holdout_rgb_preds, holdout_all_preds)

saveRDS(svm_poly_roc_data, file = "./svm_poly_roc_data.rda")
```

```{r}
# svm_poly_roc_data = readRDS("./svm_poly_roc_data.rda")
g_svm_poly <- svm_poly_roc_data %>%
  ggplot(aes(x=1 - specificity, y=sensitivity, color=Color_space)) +
  geom_line(aes(linetype = Source)) +
  theme(aspect.ratio=1)

g_svm_poly
```

```{r}
g_svm_poly_zoom <- g_svm_poly +
coord_cartesian(xlim=c(0, 0.2), ylim=c(0.5, 1)) +
guides(colour="none")

g_svm_poly_zoom
```


```{r}
# For SVM with radial basis function kernel

# Tuned model CV predictions
cv_rgb_preds <- collect_predictions(best_svm_rgb_rbf_cv) %>%
  roc_curve(truth = BT, .pred_TRUE, event_level = "first") %>%
  mutate(Source = "training_CV", Color_space = "RGB")
cv_all_preds <- collect_predictions(best_svm_all_rbf_cv) %>%
  roc_curve(truth = BT, .pred_TRUE, event_level = "first") %>%
  mutate(Source = "training_CV", Color_space = "All Predictors")

# Final model prediction
holdout_rgb_preds <- augment(final_svm_rgb_rbf, new_data = holdout_data) %>%
  roc_curve(truth = BT, .pred_TRUE, event_level = "first") %>%
  mutate(Source = "holdout", Color_space = "RGB")
holdout_all_preds <- augment(final_svm_all_rbf, new_data = holdout_data) %>%
  roc_curve(truth = BT, .pred_TRUE, event_level = "first") %>%
  mutate(Source = "holdout", Color_space = "All Predictors")

# Compute ROC
svm_rbf_roc_data <- bind_rows(cv_rgb_preds, cv_all_preds, holdout_rgb_preds, holdout_all_preds)

saveRDS(svm_rbf_roc_data, file = "./svm_rbf_roc_data.rda")
```

```{r}
# svm_rbf_roc_data = readRDS("./svm_rbf_roc_data.rda")
g_svm_rbf <- svm_rbf_roc_data %>%
  ggplot(aes(x=1 - specificity, y=sensitivity, color=Color_space)) +
  geom_line(aes(linetype = Source)) +
  theme(aspect.ratio=1)

g_svm_rbf
```

```{r}
g_svm_rbf_zoom <- g_svm_rbf +
coord_cartesian(xlim=c(0, 0.2), ylim=c(0.8, 1)) +
guides(colour="none")

g_svm_rbf_zoom
```


### Calculate Performance Metrics at the Optimal Threshold for All SVM Models (RGB)
```{r}
# 
# metrics_svm_rgb_linear <- calculate_metrics_at_threshold(
#     final_svm_rgb_linear, train_data, holdout_data,
#     "SVM with linear kernel", "RGB",
#     0.23, 0.99) %>% select(-.estimator) 
# 
# metrics_svm_rgb_poly <- calculate_metrics_at_threshold(
#     final_svm_rgb_poly, train_data, holdout_data,
#     "SVM with polynomial kernel", "RGB",
#     0.25, 0.99) %>% select(-.estimator) 
# 
# metrics_svm_rgb_rbf <- calculate_metrics_at_threshold(
#     final_svm_rgb_rbf, train_data, holdout_data,
#     "SVM with radial basis function kernel", "RGB",
#     0.31, 0.99) %>% select(-.estimator) 
# 
# ```
# ```{r}
# all_svm_metrics <- bind_rows(metrics_svm_rgb_linear,
#           metrics_svm_rgb_poly,
#           metrics_svm_rgb_rbf)
# 
# saveRDS(all_svm_metrics, file = "./all_svm_metrics.rda") 

```

```{r}
all_svm_rgb_metrics = readRDS("./all_svm_metrics.rda")
```

```{r}
all_svm_rgb_metrics %>%
  pivot_wider(names_from = .metric, values_from = .estimate) %>%
  knitr::kable(caption="Performance Metrics for Tuned SVM Models at Optimal Threshold in RGB Space", digits=3)
```


### Calculate Performance Metrics at the Optimal Threshold for All SVM Models (All features)
```{r}

# metrics_svm_all_linear <- calculate_metrics_at_threshold(
#     final_svm_all_linear, train_data, holdout_data,
#     "SVM with linear kernel", "All Predictors",
#     0.41, 0.99) %>% select(-.estimator)
# 
# metrics_svm_all_poly <- calculate_metrics_at_threshold(
#     final_svm_all_poly, train_data, holdout_data,
#     "SVM with polynomial kernel", "All Predictors",
#     0.31, 0.99) %>% select(-.estimator)
# 
# metrics_svm_all_rbf <- calculate_metrics_at_threshold(
#     final_svm_all_rbf, train_data, holdout_data,
#     "SVM with radial basis function kernel", "All Predictors",
#     0.55, 0.99) %>% select(-.estimator)

```
```{r}
# all_svm_all_metrics <- bind_rows(metrics_svm_all_linear,
#           metrics_svm_all_poly,
#           metrics_svm_all_rbf)
# 
# saveRDS(all_svm_all_metrics, file = "./all_svm_all_metrics.rda")

```

```{r}
all_svm_all_metrics = readRDS("./all_svm_all_metrics.rda")
```

```{r}
all_svm_all_metrics %>% bind_rows(all_svm_rgb_metrics) %>% 
  pivot_wider(names_from = .metric, values_from = .estimate) %>%
  arrange(dataset) |> 
  select(model, everything()) |>
  knitr::kable(caption="Performance Metrics for Tuned SVM Models at Optimal Threshold", digits=3) |> 
  kable_styling(full_width = F) |> 
  #kable_styling(position = "center") |> 
  collapse_rows(columns=3)
```











```{r}
# Stop cluster
stopCluster(cl)
registerDoSEQ()
```

```{r}
# # saving the model
# saveRDS(model, file = "C:/Users/ ... /model.rda")
# 
# #loading the model
# model_old = readRDS("C:/Users/ ... /model.rda")
```





